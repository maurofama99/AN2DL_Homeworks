# -*- coding: utf-8 -*-
"""FirstTry_Homework1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16qsr5mFBE_VQHVeJOZKZZ0nP31aSplQf

# Prima prova, primo allenamento

Obbiettivi:


*   Transfer learning
*   data augmentation
*   Split del dataset
*   Allenamento della rete con early stopping
*   Confusion matrix sul validation dataset
"""

import tensorflow as tf
import numpy as np
import os
import random
import pandas as pd
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.metrics import confusion_matrix
import cv2

import os
import time 
from PIL import Image

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torchvision.transforms import ToTensor
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split

tfk = tf.keras
tfkl = tf.keras.layers
print(tf.__version__)


# Download and import visualkeras library
!pip install visualkeras
import visualkeras

# Random seed for reproducibility
seed = 42

random.seed(seed)
os.environ['PYTHONHASHSEED'] = str(seed)
np.random.seed(seed)
tf.random.set_seed(seed)
tf.compat.v1.set_random_seed(seed)

"""# Scelta modello e Fine Tuning

Let's import the feature extraction network of the VGG16 model, adding the new classifier layers with an output layer of 8 neurons (as many as the number of classes in the dataset)
"""

# example of tending the vgg16 model
from keras.applications.vgg16 import VGG16
from keras.models import Model
from keras.layers import Dense
from keras.layers import Flatten

inputshape = [96, 96, 3]
# load model without classifier layers
convnet = VGG16(include_top=False, input_shape=(96, 96, 3))
# mark loaded layers as not trainable
for layer in convnet.layers:
	layer.trainable = True

def create_model(input_shape):
  # mark loaded layers as not trainable

  dropout_rate = 0.2
    
    # Da provare: flatten, dropout, dense (relu), dropout, dense (softmax)

    # Build the model
  input_layer = convnet.layers[0].input
  flat1 = Flatten()(convnet.layers[-1].output)
  flat1 = tfkl.Dropout(dropout_rate, seed=seed)(flat1)
  hidden_layer1 = tfkl.Dense(units=1024, activation='relu', name='Hidden1', 
                                kernel_initializer=tfk.initializers.HeUniform(seed=seed))(flat1)
  hidden_layer1 = tfkl.Dropout(dropout_rate, seed=seed)(hidden_layer1)
  output = tfkl.Dense(units=8, activation='softmax', name='Dropout', 
                                kernel_initializer=tfk.initializers.HeUniform(seed=seed))(hidden_layer1)
    # Connect input and output through the Model class
  model = tfk.Model(inputs=input_layer, outputs=output, name='model')

    # Compile the model
  learning_rate = 1e-5
  opt = tfk.optimizers.Adam(learning_rate)
  model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=opt, metrics='accuracy')

    # Return the model
  return model

# summarize

model = create_model(input_shape = inputshape)
model.summary()

tfk.utils.plot_model(model)

"""#  Splitting the dataset

Before splitting the dataset let's analyze it
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My Drive

data_dir="training_data_final"
batch_size = 16
img_height = 96
img_width = 96

def add_noise(img):
    '''Add random noise to an image'''
    VARIABILITY = 70
    deviation = VARIABILITY*random.random()
    noise = np.random.normal(0, deviation, img.shape)
    img += noise
    np.clip(img, 0., 255.)
    return img

def change_color(image):
    image = np.array(image)
    hsv_image = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)
    return hsv_image

def preprocessing(image):
    CP1 = 0.3
    CP2 = 0.6
    n = random.random()
    if n<=CP1: 
        return change_color(image)
    elif n>CP1 and n<=CP2:
        return add_noise(image)
    return image

training_dir = data_dir
from keras_preprocessing.image import ImageDataGenerator
data_gen = ImageDataGenerator(rescale=1/255, validation_split=0.2)
aug_train_data_gen = ImageDataGenerator(rotation_range=30, 
                                        height_shift_range=50, 
                                        width_shift_range=50, 
                                        zoom_range=0.3, 
                                        horizontal_flip=True, 
                                        vertical_flip=True,  
                                        fill_mode='reflect',
                                        #preprocessing_function=preprocessing,
                                        rescale=1/255, 
                                        validation_split=0.2)
train_gen = aug_train_data_gen.flow_from_directory(directory=training_dir,
                                               target_size=(96,96),
                                               color_mode='rgb',
                                               classes=None, # can be set to labels
                                               class_mode='categorical',
                                               batch_size=64,
                                               shuffle=True,
                                               seed=seed,
                                               subset="training")

valid_gen = data_gen.flow_from_directory(directory=training_dir,
                                               target_size=(96,96),
                                               color_mode='rgb',
                                               #classes=labels, # can be set to labels
                                               class_mode='categorical',
                                               batch_size=1,
                                               shuffle=True,
                                               seed=seed,
                                               subset="validation")

train_gen.reset()
finalarray = np.array([[0, 0, 0, 0, 0, 0, 0, 0]])
for batch in range(0, train_gen.__len__(), 1):
  tmpsum = np.sum([train_gen.next()[1]], axis=1)
  #print(tmpsum.shape)
  #print(finalarray.shape)
  finalarray = np.concatenate((tmpsum,finalarray), axis=0)
  #print(tmpsum)
  finalarray = np.sum([finalarray], axis=1)
  
print(finalarray)

print("total images (augmented) = ", np.sum(finalarray))

from datetime import datetime

def create_folders_and_callbacks(model_name):

  exps_dir = os.path.join('data_augmentation_experiments')
  if not os.path.exists(exps_dir):
      os.makedirs(exps_dir)

  now = datetime.now().strftime('%b%d_%H-%M-%S')

  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))
  if not os.path.exists(exp_dir):
      os.makedirs(exp_dir)
      
  callbacks = []

  # Model checkpoint
  # ----------------
  ckpt_dir = os.path.join(exp_dir, 'ckpts')
  if not os.path.exists(ckpt_dir):
      os.makedirs(ckpt_dir)

  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), 
                                                     save_weights_only=False, # True to save only weights
                                                     save_best_only=False) # True to save only the best epoch 
  callbacks.append(ckpt_callback)

  # Visualize Learning on Tensorboard
  # ---------------------------------
  tb_dir = os.path.join(exp_dir, 'tb_logs')
  if not os.path.exists(tb_dir):
      os.makedirs(tb_dir)
      
  # By default shows losses and metrics for both training and validation
  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir, 
                                               profile_batch=0,
                                               histogram_freq=1)  # if > 0 (epochs) shows weights histograms
  callbacks.append(tb_callback)

  # Early Stopping
  # --------------
  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
  callbacks.append(es_callback)

  return callbacks

callbacks = create_folders_and_callbacks(model_name='CNN')

patience = 150
early_stopping = tfk.callbacks.EarlyStopping(monitor='val_mse', mode='min', patience=patience, restore_best_weights=True)

# Train the model
tl_history = model.fit(
    train_gen,
    epochs = 200,
    validation_data = valid_gen,
    callbacks=callbacks
).history