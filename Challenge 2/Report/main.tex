\documentclass[11pt, oneside]{article} 
\usepackage{mathptmx}
\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{longtable}
\usepackage{rotating}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{caption}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[table]{xcolor}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{amsmath}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Cdot}{\boldsymbol{\cdot}}

\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{conv}{Convention}
\newtheorem{rem}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}

\font\arial=cmr12 at 40pt
\title{{\arial AN2DL Second Homework}}
\font\calibri=cmr12 at 20pt
\author{{\calibri Mauro Fam√†,   Sofia Martellozzo,   Lorenzo Mondo\\ \\
        Group cANNoli}}
\date{Academic Year 2022-2023}

\begin{document}

\maketitle
\begin{center}
    \includegraphics[scale=0.43]{images/title.png}
\end{center}
\newpage
\vspace{.25in}

%---------------------------------------%

\section{Introduction}
This report describes a multivariate time series classification project in which several artificial neural network models were used. Specifically, we conducted training using LSTM (Long Short-Term Memory), BiLSTM (Bidirectional Long Short-Term Memory), and 1D Convolutional Neural Networks (CNNs). The results obtained were compared and analyzed to identify the most performant model for this type of problem. We also explored the impact of different model configurations on their performance.

The dataset used for this project consists of multivariate time series with 6 variables and a total of 12 classes: "Wish," "Another," "Comfortably," "Money," "Breathe," "Time," "Brain," "Echoes," "Wearing," "Sorrow," "Hey," and "Shine." The class with the most samples is "Sorrow" with 777 samples, while the class with the fewest samples is "Wish" with 34 samples. The dataset is heavily imbalanced, with some classes being much more common than others. In some experiments, we used data augmentation techniques to augment the time series and to balance the dataset with oversampling.

Prior to analysis, in some experiments, the time series were pre-processed to remove missing values and standardize the data. This report provides an overview of the techniques used and a detailed description of the results obtained. In addition, some concluding thoughts are presented on the advantages and disadvantages of the different models and on the factors that influence their performance.

%---------------------------------------%
\section{Techniques}
\subsection{Data Augmentation}

\subsection{Pre-processing}
Normalization and standardization are two techniques used to prepare data for analysis. Both techniques are used to standardize the values of variables in order to make them comparable to each other, but they differ in how they are performed.
\subsubsection{Normalization}
Normalization consists of transforming the values of each variable into a specific range, usually between 0 and 1. One way to perform normalization is through min-max normalization, which scales the values of each variable between a minimum and maximum value. The formula for min-max normalization is as follows:
\[ x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}} \]\
where x is the value to be normalized, $x_{min}$ is the minimum value of the variable, $x_{max}$ is the maximum value of the variable, and $x_{norm}$ is the normalized value.
Normalization is often used when it is important to maintain the original range of values or when you want to avoid privileging some variables over others.
\subsubsection{Standardization}
Standardization, on the other hand, consists of transforming the values of each variable into a standard scale, where the mean is 0 and the standard deviation is 1. Standardization is performed using the following formula:
\[x_{std} = \frac{x - \mu}{\sigma} \]
where x is the value to be standardized, $\mu$ is the mean of the variable, $\sigma$ is the standard deviation of the variable, and $x_{std}$ is the standardized value.
Standardization is often used when it is important to standardize values on a standard scale or when you want to avoid variables with high values from having too much influence on the analysis.\\\\
In our case, model training performed better using standardization instead of normalization for several reasons. First, standardization is more suited to our data as the values of the variables have a normal distribution. In addition, standardization allows us to avoid privileging some variables over others, as all variables are standardized on the same scale. Finally, standardization enables us to use all the information available in our data, as normalization does not limit variables' values to a specific range.

%---------------------------------------%
\section{Development Models}
In our experiments, we split the dataset into train and validation sets using an 80/20 split, maintaining the proportion of time series for each class in both sets. We used the train set to train the models and the validation set to evaluate their performance.\\
In some cases, we trained the models without any preprocessing of the data. In other cases, we applied pre-processing techniques such as normalization or standardization to the data before training the models. In some cases, we also used data augmentation techniques to generate additional time series data to train the models.
\subsection{LSTM (Long Short-Term Memory)}
A particular type of artificial neural network that succeeds in processing sequential data is the LSTM. In addition to having input and output gates, LSTM cells also have a cell state that has the ability to hold data for extended periods of time. While the cell state enables the LSTM to retain significant information for extended periods of time, the input and output gates regulate the flow of information into and out of the cell. In order to recall pertinent information from earlier in the sequence and utilize it to forecast future events, LSTMs can leverage their cell state when processing sequential data. This makes LSTMs particularly useful for tasks like multivariate time series classification, where long-term dependencies between elements in the time series are important.\\
For this multivariate time series classification problem, we combined a convolutional neural network with a Long Short-Term Memory (LSTM) neural network.
The network starts with an input layer that has the shape of the multivariate time series. Next, a bidirectional LSTM layer is used, which is a type of LSTM that uses two separate LSTMs, one for the forward flow of the time series and the other for the backward flow. This layer is used with the \"return\_sequences\" parameter set to True, which means it will return a sequence of outputs rather than a single output for each timestep.
After the bidirectional LSTM layer, a 1D convolutional layer is used, followed by a 1D pooling layer to reduce the size of the extracted features. Next, another bidirectional LSTM layer is used, followed by another Conv1D layer and a global 1D pooling layer.
After these layers, a dropout layer is used to prevent overfitting of the model during training. Next, a densely connected layer is used with a number of units equal to the number of channels in the output shape, followed by a reshape layer to obtain a tensor of the desired output shape. Finally, a Conv1D layer is used to perform the actual prediction of the multivariate time series.\\
We trained this model using pre-processed input with the data augmentation technique described in Section 2.1, obtaining an accuracy of 0.6809 in the final phase of the competition.
\subsection{BiLSTM (Bidirectional Long Short-Term Memory)}
BiLSTM is a variant of LSTM that processes sequential data in both forward and backward directions. This allows BiLSTM to capture dependencies between elements in the sequence both forwards and backward, which can be especially useful for tasks like multivariate time series classification where both long-term and short-term dependencies are important. In a BiLSTM model, the input data is processed by two separate LSTM networks, one that processes the data from the beginning to the end of the sequence and one that processes the data from the end to the beginning. The outputs of these two networks are then combined to produce a final prediction or classification.\\\\
In our experiments we played around with several BiLSTM model setups, varying the amount of neurons in the layers and using preprocessing and data augmentation methods. In some instances, we trained the model directly without any preprocessing, whereas in others, we normalized or standardized the data before training the model. In other instances, we also created extra time series data for the model's training using approaches for data augmentation.
The BiLSTMs model setup without any preprocessing or data augmentation had the best accuracy performance on the test set. This set-up, which had two BiLSTM layers and 128 neurons in each layer, had a test-set accuracy of 68\%.

\subsection{1D Convolutional Neural Networks (CNNs)}
1D Convolutional Neural Networks are a type of artificial neural network that is particularly effective at extracting features from sequential data. CNNs use convolutional filters to learn patterns in the data and use those patterns to make predictions or decisions. In a 1D CNN, the filters are applied to the input data along the time dimension, allowing the model to learn temporal patterns in the data. This makes 1D CNNs well-suited for tasks like multivariate time series classification, where the relationships between different time series are important for making accurate predictions.\\
In our experiments, 
%---------------------------------------%
\section{Final Model}
\subsection{ResNet}
The network uses 1D convolutional layers to extract features from the time series and then uses batch normalization layers and ReLU activation layers to improve the model's performance.\\
The network consists of three blocks, each of which consists of three Conv1D layers and a batch normalization layer. Each block also uses a shortcut layer which adds the input of the block to the output of the third Conv1D layer of the block. This allows the network to "skip" directly to the output of the block without going through all of its intermediate layers, which can help reduce the risk of the model's accuracy degrading due to excessive expansion of its depth.\\
Finally, the network uses a 1D global pooling layer to reduce the size of the extracted features and a densely connected layer to perform the time series classification.
%---------------------------------------%
\section{Conclusions}

%---------------------------------------%
\section*{References}
[0] \href{https://doi.org/10.48550/arXiv.1712.04621}{Perez, L., \& Wang, J. (2017). The Effectiveness of Data Augmentation in Image Classification using Deep Learning. arXiv.}\\
\end{document}
